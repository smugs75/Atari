{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections      import deque\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Activation, Permute, Dense, Dropout, Conv2D, Input, Lambda, Flatten\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrappers import  wrap_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred):\n",
    "        return tf.losses.huber_loss(y_true,y_pred)\n",
    "\n",
    "class Agent():\n",
    "       \n",
    "    def __init__(self, state_size, action_size, weights=None):\n",
    "        \n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.model = self.build_model()\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = (1-0.01) / 1000000\n",
    "        self.gamma = 0.99\n",
    "        self.target_model = self.build_model()\n",
    "        self.next_target_update = 10000\n",
    "        self.nb_do_nothing = 0\n",
    "        if weights != None:\n",
    "            print(\"ok\")\n",
    "            self.model.load_weights(weights)\n",
    "            self.target_model.load_weights(weights)\n",
    "        self.current_state = []\n",
    "    \n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "        \n",
    "    def build_model(self):       \n",
    "        \n",
    "        input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "        model = Sequential()\n",
    "\n",
    "        # (width, height, channels)\n",
    "        model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.add(Activation('linear'))\n",
    "        print(model.summary())\n",
    "        #model.summary()\n",
    "        \n",
    "        model.compile(RMSprop(lr=.00025, rho=0.95), loss='mae')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "        \n",
    "    def next_action(self, current_state):\n",
    "        batch = self.process_state_batch(np.expand_dims(current_state, axis=0))\n",
    "        q_values = self.model.predict(batch)[0]\n",
    "        \n",
    "        if self.nb_do_nothing > 29:\n",
    "            if np.random.rand() <= self.exploration_rate:\n",
    "                action = random.randrange(self.action_size - 1) + 1  \n",
    "            else:\n",
    "                action = np.argmax(q_values[1:])\n",
    "                \n",
    "        else:\n",
    "            if np.random.rand() <= self.exploration_rate:\n",
    "                action = random.randrange(self.action_size)  \n",
    "            else:\n",
    "                action = np.argmax(q_values)\n",
    "        if action == 0:\n",
    "            self.nb_do_nothing +=1\n",
    "            \n",
    "        return action, np.max(q_values)\n",
    "    \n",
    "\n",
    "    def update_frames(self, frames, next_frame):\n",
    "        return np.vstack((frames[1:,:,:], [next_frame]))\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        batch_states, batch_target = [], []\n",
    "        if len(self.memory) < batch_size:\n",
    "                return\n",
    "        sample_batch = random.sample(self.memory, batch_size)\n",
    "        for current_state, action, reward, next_state, done in sample_batch:\n",
    "            state = self.process_state_batch(np.expand_dims(current_state, axis=0))\n",
    "            target = self.target_model.predict(state)\n",
    "            \n",
    "            next_batch = self.update_frames(current_state, next_state)\n",
    "            next_batch = self.process_state_batch(np.expand_dims(next_batch, axis=0))\n",
    "            q_next_state = np.max(self.target_model.predict(next_batch)[0])\n",
    "            if not done:\n",
    "                target[0][action] = reward + self.gamma * q_next_state\n",
    "            else:\n",
    "                target[0][action] = reward\n",
    "            batch_states.append(state[0])\n",
    "            batch_target.append(target[0])\n",
    "        \n",
    "        self.model.fit(np.array(batch_states), np.array(batch_target), epochs=1, verbose=0)\n",
    "        self.decay_exploration_rate()\n",
    "        \n",
    "\n",
    "            \n",
    "    def update_target_model(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] =  weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "                \n",
    "            \n",
    "    def decay_exploration_rate(self):\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate - self.exploration_decay)\n",
    "        \n",
    "    def initialize_memory(self):\n",
    "        print(\"initializing memory\")\n",
    "        while len(self.memory) < 50000:\n",
    "            self.nb_do_nothing = 0\n",
    "            obs = self.process_observation(env.reset())\n",
    "            current_state = np.array([obs, obs, obs, obs])\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = random.randrange(self.action_size)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state =  self.process_observation(next_state)\n",
    "                self.remember(current_state, action, reward, next_state, done)\n",
    "                current_state = self.update_frames(current_state, next_state)\n",
    "        print(\"memory initialized\")\n",
    "    \n",
    "    def learn(self, env, nb_episodes, initialize=True, batch_size=32):\n",
    "        if initialize:\n",
    "            self.initialize_memory()\n",
    "        steps = []\n",
    "        all_rewards = []\n",
    "        pbar = tqdm(range(nb_episodes))\n",
    "        total_steps = 0\n",
    "        for i in pbar:\n",
    "            self.nb_do_nothing = 0\n",
    "            obs = self.process_observation(env.reset())\n",
    "            current_state = np.array([obs, obs, obs, obs])\n",
    "            episode_nb_steps = 0\n",
    "            episode_reward = 0\n",
    "            episode_sum_q_value = 0\n",
    "            done = False\n",
    "            actions = {0:0, \n",
    "                      1:0,\n",
    "                      2:0, \n",
    "                      3:0}\n",
    "            \n",
    "            \n",
    "            while not done:\n",
    "                total_steps += 1\n",
    "                if total_steps % 10000 == 0:\n",
    "                    self.update_target_model()\n",
    "                action, q_value = self.next_action(current_state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                reward = self.process_reward(reward)\n",
    "                \n",
    "                actions[action] += 1\n",
    "                episode_sum_q_value += q_value\n",
    "                episode_nb_steps += 1\n",
    "                episode_reward += reward\n",
    "                \n",
    "                next_state = self.process_observation(next_state)\n",
    "                self.remember(current_state, action, reward, next_state, done)\n",
    "                self.replay(batch_size)\n",
    "                current_state = self.update_frames(current_state, next_state)\n",
    "                \n",
    "            all_rewards.append(episode_reward)\n",
    "            steps.append(episode_nb_steps)\n",
    "            for action in actions:\n",
    "                actions[action] =  str(round(actions[action] / episode_nb_steps * 100, 0)) + \"%\"\n",
    "            pbar.set_description(\"episode: %s, nb_steps_this_episode: %s, reward: %s, exploration: %s, q_value_mean: %s, actions: %s, total_steps: %s\" \\\n",
    "                                 % (i, episode_nb_steps, episode_reward, self.exploration_rate, episode_sum_q_value / episode_nb_steps, actions, total_steps))\n",
    "            if (total_steps + 1) % 250000 == 0:\n",
    "                path = 'weights/my_dqn_model_weights_' + str(total_steps + 1)\n",
    "                self.model.save(path)\n",
    "            \n",
    "        return steps, all_rewards\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v4')\n",
    "env  = wrap_dqn(env)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing memory\n",
      "WARN: <class 'wrappers.NoopResetEnv'> doesn't implement 'step' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\n",
      "memory initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 47665, nb_steps_this_episode: 4, reward: 0.0, exploration: 0.5043891700107592, q_value_mean: 0.01410368806682527, actions: {0: '25.0%', 1: '50.0%', 2: '0.0%', 3: '25.0%'}, total_steps: 500617:  48%|████▊     | 47666/100000 [17:48:30<39:59:15,  2.75s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-171173673fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f3c57fedb613>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, env, nb_episodes, initialize, batch_size)\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m                 \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f3c57fedb613>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mq_next_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq_next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_GYM_36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_GYM_36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_GYM_36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_GYM_36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_GYM_36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps, rewards = agent.learn(env, 100000)\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "agent.exploration_rate = 0.1557289900156853\n",
    "agent.model.load_weights(\"weights/dqn_Breakout-v4_weights_1750000.h5f\")\n",
    "#steps, rewards = agent.learn(env, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heat_map(agent, action, x, state):\n",
    "    model = agent.model\n",
    "    \n",
    "    predictiont_output = model.output[:, action]\n",
    "    last_conv_layer = model.get_layer('conv2d_3')\n",
    "    grads = K.gradients(predictiont_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "    batch = agent.process_state_batch(np.expand_dims(x, axis=0))\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([batch])\n",
    "    for i in range(7):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    \n",
    "   # heatmap = heatmap / np.max(heatmap)\n",
    "   # print(heatmap)\n",
    "   # print(\"\")\n",
    "   # print(pooled_grads_value)\n",
    "   # print(\"\")\n",
    "   # print(conv_layer_output_value)\n",
    "   # print(\"\")\n",
    "   # print(x)\n",
    "   # print(\"\")\n",
    "   # print(np.max(batch))\n",
    "    heatmap = cv2.resize(heatmap, (160, 210))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + state\n",
    "    return superimposed_img /255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def distant_render(env, agent, state_size, nb_steps=200):\n",
    "    agent.exploration_rate = 0\n",
    "    state_img = env.reset()\n",
    "    previous_state = state_img\n",
    "    img = plt.imshow(state_img)\n",
    "    state = agent.process_observation(state_img)\n",
    "    current_state = np.array([state, state, state, state])\n",
    "    \n",
    "    done = False\n",
    "     # only call this once\n",
    "    #for i in range(nb_steps):\n",
    "    while not done:\n",
    "              \n",
    "        \n",
    "        action, _ = agent.next_action(current_state)\n",
    "        \n",
    "        img.set_data(state_img) # just update the data\n",
    "        plt.title(actions[action])\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state_img = get_heat_map(agent, action, current_state, previous_state)\n",
    "        previous_state = next_state\n",
    "        next_state = agent.process_observation(next_state)\n",
    "        current_state = agent.update_frames(current_state, next_state)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#agent = Agent(state_size, action_size, weights=\"./weights/DQN_breakout_model.h5\")\n",
    "distant_render(env, agent, state_size, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save_weights(\"./weights/My_DQN_11-10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
