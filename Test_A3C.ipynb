{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch, os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNPolicy(torch.nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.lstm = nn.LSTMCell(32 * 5 * 5, 256)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(256, 1), nn.Linear(256, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), (hx, cx)\n",
    "\n",
    "    def try_load(self, save_dir, checkpoint='*.tar'):\n",
    "        paths = glob.glob(save_dir + checkpoint) ; step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split('.')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step is 0 else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: weights/strong.40.tar\n"
     ]
    }
   ],
   "source": [
    "save_dir =\"weights/\"\n",
    "env = gym.make('Breakout-v4')\n",
    "num_actions = env.action_space.n # get the action space of this game\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255.\n",
    "\n",
    "\n",
    "model = NNPolicy(channels=1, num_actions=num_actions)\n",
    "model.try_load(save_dir)\n",
    "hx, cx = Variable(torch.zeros(1, 256)), Variable(torch.zeros(1, 256))\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/Pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    state = torch.tensor(prepro(state))\n",
    "    value, logit, (hx, cx) = model((state.view(1,1,80,80), (hx, cx)))\n",
    "    logp = F.log_softmax(logit, dim=-1)\n",
    "    action = torch.exp(logp).multinomial(num_samples=1).data[0]\n",
    "    state, reward, done, info = env.step(action.numpy()[0])\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(state)\n",
    "done = False\n",
    " # only call this once\n",
    "while not done:\n",
    "    state = torch.tensor(prepro(state))\n",
    "    value, logit, (hx, cx) = model((state.view(1,1,80,80), (hx, cx)))\n",
    "    logp = F.log_softmax(logit, dim=-1)\n",
    "    action = torch.exp(logp).multinomial(num_samples=1).data[0]\n",
    "    state, reward, done, info = env.step(action.numpy()[0])\n",
    "    img.set_data(state) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    #cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutputs():\n",
    "\t\"\"\" Class for making a forward pass, and getting:\n",
    "\t1. The network output.\n",
    "\t2. Activations from intermeddiate targetted layers.\n",
    "\t3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\tdef __init__(self, model, target_layers):\n",
    "\t\tself.model = model\n",
    "\t\tself.feature_extractor = FeatureExtractor(self.model.features, target_layers)\n",
    "\n",
    "\tdef get_gradients(self):\n",
    "\t\treturn self.feature_extractor.gradients\n",
    "\n",
    "\tdef __call__(self, x):\n",
    "\t\ttarget_activations, output  = self.feature_extractor(x)\n",
    "\t\toutput = output.view(output.size(0), -1)\n",
    "\t\toutput = self.model.classifier(output)\n",
    "\t\treturn target_activations, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model, target_layer_names, use_cuda=False):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input) \n",
    "\n",
    "    def __call__(self, input, index = None):\n",
    "        if self.cuda:\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_variables=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        weights = np.mean(grads_val, axis = (2, 3))[0, :]\n",
    "        cam = np.zeros(target.shape[1 : ], dtype = np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam = GradCam(model = models.vgg19(pretrained=True), target_layer_names = [\"conv4\"], use_cuda=args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /home/david/.torch/models/vgg19-dcbb9e9d.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "m = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNPolicy(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (gru): GRUCell(800, 256)\n",
      "  (critic_linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (actor_linear): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
